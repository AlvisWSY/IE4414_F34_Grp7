{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["from __future__ import print_function, division\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","\n","import torchvision.transforms.functional as F\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, Subset\n","import numpy as np\n","import torchvision\n","from torchvision import models, transforms\n","from torchvision.datasets.folder import make_dataset\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report\n","\n","%matplotlib inline\n","plt.ion()   # interactive mode"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pip install torchsummary"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from torchsummary import summary"]},{"cell_type":"markdown","metadata":{},"source":["### To test for pre-trained\n","1. Inception + Adam\n","2. VGG adam optimiser debug (if any)"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Loading data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define the dataset class\n","class sg_food_dataset(torch.utils.data.dataset.Dataset):\n","    def __init__(self, root, class_id, transform=None):\n","        self.class_id = class_id\n","        self.root = root\n","        all_classes = sorted(entry.name for entry in os.scandir(root) if entry.is_dir())\n","        if not all_classes:\n","            raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n","        self.classes = [all_classes[x] for x in class_id]\n","        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n","\n","        self.samples = make_dataset(self.root, self.class_to_idx, extensions=('jpg'))\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        path, target = self.samples[idx]\n","        with open(path, \"rb\") as f:\n","            sample = Image.open(f).convert('RGB')\n","        if self.transform is not None:\n","            sample = self.transform(sample)\n","        return sample, target"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Data augmentation and normalization for training\n","data_transforms = {\n","    'train': transforms.Compose([\n","        # Define data preparation operations for training set here.\n","        # Tips: Use torchvision.transforms\n","        #       https://pytorch.org/vision/stable/transforms.html\n","        #       Normally this should at least contain resizing (Resize) and data format converting (ToTensor).\n","        transforms.RandomResizedCrop(224),\n","        transforms.ColorJitter(brightness=0.1, contrast=0.1 , saturation = 0.1), #random brightness, contrast etc\n","        transforms.GaussianBlur(kernel_size=(15, 15)),\n","        transforms.ColorJitter(brightness=0.2, contrast=0.3 , saturation = 0.2, hue=0.3), #random brightness, contrast etc\n","        transforms.RandomVerticalFlip(),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet prior\n","    ]),\n","    'val': transforms.Compose([\n","        # Define data preparation operations for testing/validation set here.\n","        transforms.Resize(256, antialias=True),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet prior\n","    ]),\n","}\n","\n","data_dir = './sg_food' \n","subfolder = {'train': 'train', 'val': 'val'}\n","\n","# Define the dataset\n","selected_classes = [0, 2, 4, 7, 9]\n","n_classes = len(selected_classes)\n","image_datasets = {x: sg_food_dataset(root=os.path.join(data_dir, subfolder[x]),\n","                                     class_id=selected_classes,\n","                                     transform=data_transforms[x]) \n","                  for x in ['train', 'val']}\n","class_names = image_datasets['train'].classes\n","print('selected classes:\\n    id: {}\\n    name: {}'.format(selected_classes, class_names))\n","\n","# Define the dataloader 64=default, 128=adam\n","batch_size = 64\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n","                                             shuffle=True, num_workers=0)\n","              for x in ['train', 'val']}\n","\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Visualizing the dataset\n","Fetch a batch of training data from the dataset and visualize them. \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def imshow(inp, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    inp = std * inp + mean\n","    inp = np.clip(inp, 0, 1)\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","\n","\n","# Get a batch of training data\n","inputs, classes = next(iter(dataloaders['train']))\n","\n","# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs[:4])\n","\n","imshow(out, title=[class_names[x] for x in classes[:4]])"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Model Initailization"]},{"cell_type":"markdown","metadata":{},"source":["### 3.1 Define Models"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load the pre-trained VGG16 model\n","vgg16 = models.vgg16(pretrained=True)\n","\n","# Load the pre-trained VGG19 model\n","vgg19 = models.vgg19(pretrained=True)\n","\n","# Load the pre-trained ResNet-18 model\n","resnet18 = models.resnet18(pretrained=True)\n","\n","# Load the pre-trained Inception v3 model\n","inception_v3 = models.inception_v3(pretrained=True)\n","\n","# Load the pre-trained ResNet-50 model\n","resnet50 = models.resnet50(pretrained=True)\n"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2 Modify Models"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Set num of classes\n","num_classes = 5\n","\n","# Modify VGG16\n","vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, num_classes)\n","\n","# Modify VGG19\n","vgg19.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, num_classes)\n","\n","# Modify ResNet18\n","resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n","\n","# Modify Resnet50\n","resnet50.fc = nn.Linear(resnet50.fc.in_features, num_classes)\n","\n","# Modify Inception_v3\n","inception_v3.fc = nn.Linear(inception_v3.fc.in_features, num_classes)\n","inception_v3.AuxLogits.fc = nn.Linear(inception_v3.AuxLogits.fc.in_features, num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Summary\n","\n","# VGG16\n","summary(vgg16.to(device), input_size=(3, 224, 224))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# VGG19\n","summary(vgg19.to(device), input_size=(3, 224, 224))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# ResNet18\n","summary(resnet18.to(device), input_size=(3, 224, 224))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Resnet 50\n","summary(resnet50.to(device), input_size=(3, 224, 224))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Inception_v3\n","summary(inception_v3.to(device), input_size=(3, 299, 299))"]},{"cell_type":"markdown","metadata":{},"source":["### 3.3 Dict for Models"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["models_dict = {\n","    #\"VGG16\": vgg16,\n","    #\"VGG19\": vgg19,\n","    #\"ResNet18\": resnet18,\n","    \"ResNet50\":resnet50,\n","    #\"Inception_v3\": inception_v3\n","}"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train_model(model, dataloaders, criterion, optimizer, num_epochs, is_inception=False):\n","    since = time.time()\n","\n","    val_acc_history = []\n","    \n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0                \n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                \n","                if is_inception and phase == 'train':\n","                    inputs = torch.stack([F.resize(input, size=(299, 299)) for input in inputs])\n","\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    # Special case for inception because in training it has an auxiliary output. In train\n","                    #   mode we calculate the loss by summing the final output and the auxiliary output\n","                    #   but in testing we only consider the final output.\n","                    if is_inception and phase == 'train':\n","                        outputs, aux_outputs = model(inputs)\n","                        loss1 = criterion(outputs, labels)\n","                        loss2 = criterion(aux_outputs, labels)\n","                        loss = loss1 + 0.4*loss2\n","                    else:\n","                        outputs = model(inputs)\n","                        loss = criterion(outputs, labels)\n","\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","            if phase == 'val':\n","                val_acc_history.append(epoch_acc)\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, val_acc_history\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def visualize_training(val_acc_history):\n","    plt.figure()\n","    # 确保将 tensor 转移到 CPU\n","    val_acc_history_cpu = [h.cpu() for h in val_acc_history]\n","    plt.plot(val_acc_history_cpu)\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Validation Accuracy\")\n","    plt.title(\"Training History\")\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Loop through each model for training\n","for model_name, model in models_dict.items():\n","    print(f\"Training {model_name}...\")\n","    \n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    model = model.to(device)\n","    \n","    # Define Criterion\n","    criterion = nn.CrossEntropyLoss()\n","    \n","    # Define Optimizer\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","    \n","    # Inception_v3\n","    #is_inception = (model_name == \"Inception_v3\")\n","    \n","    trained_model, val_acc_history = train_model(model, dataloaders, criterion, optimizer, num_epochs=25)\n","    \n","    # Visualize Training Result\n","    visualize_training(val_acc_history)\n","    \n","    # Save Checkpoints\n","    torch.save(trained_model.state_dict(), f\"./models/{model_name}_model_Adam.pth\")"]},{"cell_type":"markdown","metadata":{},"source":["**Train Swin Transformer (tiny) with lr scheduler**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import timm\n","from timm.loss import LabelSmoothingCrossEntropy # This is better than normal nn.CrossEntropyLoss"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["HUB_URL = \"SharanSMenon/swin-transformer-hub:main\"\n","MODEL_NAME = \"swin_tiny_patch4_window7_224\"\n","# check hubconf for more models.\n","model = torch.hub.load(HUB_URL, MODEL_NAME, pretrained=True) # load from torch hub"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for param in model.parameters(): #freeze model\n","    param.requires_grad = False\n","print(model.head)\n","n_inputs = model.head.in_features\n","model.head = nn.Sequential(\n","    nn.Linear(n_inputs, 512),\n","    nn.ReLU(),\n","    nn.Dropout(0.3),\n","    nn.Linear(512, 5)\n",")\n","model = model.to(device)\n","print(model.head)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["criterion = LabelSmoothingCrossEntropy()\n","criterion = criterion.to(device)\n","optimizer = optim.AdamW(model.head.parameters(), lr=0.001)\n","# lr scheduler\n","exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.97)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs):\n","    since = time.time()\n","\n","    val_acc_history = []\n","    \n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0                \n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    loss = criterion(outputs, labels)\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","            \n","            if phase == 'train':\n","                scheduler.step()\n","                \n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","            if phase == 'val':\n","                val_acc_history.append(epoch_acc)\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, val_acc_history\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trained_model, val_acc_history = train_model(model, dataloaders, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n","    \n","# Visualize Training Result\n","visualize_training(val_acc_history)\n","    \n","# Save Checkpoints\n","torch.save(trained_model.state_dict(), f\"./data/swin_tiny.pth\")"]},{"cell_type":"markdown","metadata":{},"source":["### 4.2 Finetuning Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# 5. Prediction and Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load the pre-trained Swin_tiny model\n","#HUB_URL = \"SharanSMenon/swin-transformer-hub:main\"\n","\n","#MODEL_NAME = \"swin_tiny_patch4_window7_224\"\n","#swin_tiny = torch.hub.load(HUB_URL, MODEL_NAME, pretrained=True)\n","\n","# Load the pre-trained VGG16 model\n","vgg16 = models.vgg16(pretrained=True)\n","\n","# Load the pre-trained ResNet-18 model\n","resnet18 = models.resnet18(pretrained=True)\n","\n","# Load the pre-trained Inception v3 model\n","inception_v3 = models.inception_v3(pretrained=True)\n","\n","#Load the pre-trained VGG19 model\n","vgg19 = models.vgg19(pretrained=True)\n","\n","#Load the pre-trained ResNet50 model\n","resnet50 = models.resnet50(pretrained=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Set num of classes\n","num_classes = 5\n","\n","# Modify VGG16\n","vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, num_classes)\n","\n","# Modify VGG19\n","vgg19.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, num_classes)\n","\n","# Modify ResNet18\n","resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n","\n","# Modify Resnet50\n","resnet50.fc = nn.Linear(resnet50.fc.in_features, num_classes)\n","\n","# Modify Inception_v3\n","inception_v3.fc = nn.Linear(inception_v3.fc.in_features, num_classes)\n","inception_v3.AuxLogits.fc = nn.Linear(inception_v3.AuxLogits.fc.in_features, num_classes)\n","\n","# Modify Swin_tiny\n","'''\n","swin_tiny.head = nn.Sequential(\n","    nn.Linear(768, 512),\n","    nn.ReLU(),\n","    nn.Dropout(0.3),\n","    nn.Linear(512, num_classes)\n",")\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["models_dict = {\n","    \"VGG16\": vgg16,\n","    \"ResNet18\": resnet18,\n","    #\"Inception_v3\": inception_v3,\n","    #\"SwinTransformer_tiny\": swin_tiny\n","    #\"VGG19\": vgg19,\n","    #\"ResNet50\": resnet50,\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Data normalization for testing\n","test_data_transform = transforms.Compose([\n","        # Define data preparation operations for testing/validation set here.\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet prior\n","    ])\n","\n","data_dir = './sg_food' \n","\n","# Define the dataset\n","selected_classes = [0, 2, 4, 7, 9]\n","n_classes = len(selected_classes)\n","test_image_dataset = sg_food_dataset(root=os.path.join(data_dir, 'test'),\n","                                     class_id=selected_classes,\n","                                     transform=test_data_transform) \n","class_names = test_image_dataset.classes\n","print('selected classes:\\n    id: {}\\n    name: {}'.format(selected_classes, class_names))\n","\n","# Define the dataloader 64=default, 128=adam\n","batch_size = 128\n","test_dataloader = torch.utils.data.DataLoader(test_image_dataset, batch_size=batch_size,\n","                                             shuffle=True, num_workers=0)\n","\n","test_dataset_size = len(test_image_dataset)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def imshow(inp, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    inp = std * inp + mean\n","    inp = np.clip(inp, 0, 1)\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","\n","\n","# Get a batch of training data\n","inputs, classes = next(iter(test_dataloader))\n","\n","# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs[:4])\n","\n","imshow(out, title=[class_names[x] for x in classes[:4]])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["best_model_paths = {\n","    \"VGG16\": \"./models/VGG16_model_Adam.pth\",\n","    #\"ResNet18\": \"./models/ResNet18_model_adam.pth\",\n","    #\"Inception_v3\": \"/kaggle/input/inception_v3/pytorch/v1/1/Inception_v3_model.pth\",\n","    #\"SwinTransformer_tiny\": \"/kaggle/input/swin_tiny/pytorch/v1/1/swin_tiny.pth\"\n","    #\"VGG19\": \"./models/VGG19_model_adam.pth\",\n","    #\"ResNet50\": \"./models/ResNet50_model_adam.pth\",\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","def evaluate(model,test_dataloader,class_names):\n","    y_true = []\n","    y_pred = []\n","    with torch.no_grad():\n","        for data in test_dataloader:\n","            images, labels = data\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            c = (predicted == labels).squeeze()\n","            for i in range(len(labels)):   \n","                label = labels[i]\n","                #for confusion matrix\n","                y_true.append(label.to('cpu'))\n","                y_pred.append(predicted[i].to('cpu'))\n","                \n","        \n","    print(classification_report(y_true, y_pred, target_names=class_names))\n","    \n","    matrix = confusion_matrix(y_true, y_pred)\n","    disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=[\"BCM\",\"CR\",\"KTE\",\"OO\",\"RP\"])\n","    disp.plot()\n","    plt.title(\"confusion matrix\")\n","    plt.show()\n","    \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for model_name, model in models_dict.items():\n","    if model_name == \"VGG19\":\n","        continue\n","    print(f\"Testing {model_name}...\")\n","    \n","    model.to(device)\n","    model.load_state_dict(torch.load(best_model_paths[model_name]))\n","    model.eval()\n","    evaluate(model,test_dataloader,class_names)\n","    "]},{"cell_type":"markdown","metadata":{},"source":["### advanced 1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load ResNet18\n","model = models.resnet18(pretrained=False)\n","\n","# Modify Model, Remove FC Layer\n","num_ftrs = model.fc.in_features\n","model.fc = torch.nn.Identity()\n","\n","# Load pretrained weight\n","model.load_state_dict(torch.load(\"/kaggle/input/resnet18-v1/pytorch/v1/1/ResNet18_model.pth\"),strict = False)\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class sg_food_dataset(torch.utils.data.Dataset):\n","    def __init__(self, root, class_id, transform=None):\n","        self.class_id = class_id\n","        self.root = root\n","        all_classes = sorted(entry.name for entry in os.scandir(root) if entry.is_dir())\n","        if not all_classes:\n","            raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n","        \n","        # 创建新的类别列表，包括指定的类别和一个\"Other\"类别\n","        self.classes = [all_classes[x] for x in class_id] + ['Other']\n","        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes[:-1])}\n","        self.class_to_idx['Other'] = len(self.classes) - 1  # 'Other'类别的索引\n","\n","        # 更新make_dataset函数调用，以支持\"Other\"类别\n","        self.samples = self.make_dataset(self.root, self.class_to_idx, extensions=('jpg'), class_id=class_id)\n","        self.transform = transform\n","\n","    def make_dataset(self, directory, class_to_idx, extensions=None, class_id=None):\n","        instances = []\n","        directory = os.path.expanduser(directory)\n","        both_none = extensions is None\n","        if not both_none:\n","            def is_valid_file(x):\n","                return x.lower().endswith(extensions)\n","        for target_class in sorted(class_to_idx.keys()):\n","            class_index = class_to_idx[target_class]\n","            target_dir = os.path.join(directory, target_class)\n","            if not os.path.isdir(target_dir):\n","                continue\n","            for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n","                for fname in sorted(fnames):\n","                    path = os.path.join(root, fname)\n","                    if both_none or is_valid_file(path):\n","                        # 若当前类别不在selected_classes中，将其归为\"Other\"\n","                        if class_index not in class_id:\n","                            instances.append((path, class_to_idx['Other']))\n","                        else:\n","                            instances.append((path, class_index))\n","        return instances\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# 数据目录和子目录保持不变\n","data_dir = '/kaggle/input/sg-food/sg_food' \n","subfolder = {'train': 'train', 'val': 'val'}\n","\n","# 选定的类别索引不变，因为sg_food_dataset类内部会处理Other类别\n","selected_classes = [0, 2, 4, 7, 9]\n","\n","# 定义数据集时保持原有逻辑\n","image_datasets = {x: sg_food_dataset(root=os.path.join(data_dir, subfolder[x]),\n","                                     class_id=selected_classes,\n","                                     transform=data_transforms[x]) \n","                  for x in ['train', 'val']}\n","\n","# 更新类别名称的获取逻辑，现在包括了\"Other\"类别\n","class_names = image_datasets['train'].classes  # 现在这里已经包含了\"Other\"\n","print('Selected classes:\\n    ID: {}\\n    Name: {}'.format(selected_classes + ['Other'], class_names))\n","\n","# 定义数据加载器和设备选择的逻辑保持不变\n","batch_size = 64\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n","                                             shuffle=True, num_workers=0)\n","              for x in ['train', 'val']}\n","\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### Vision transformer exploration"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torchvision\n","from torchvision import transforms\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","train_ds = torchvision.datasets.ImageFolder('./sg_food/train/', transform=transform)\n","valid_ds = torchvision.datasets.ImageFolder('./sg_food/val/', transform=transform)\n","test_ds = torchvision.datasets.ImageFolder('./sg_food/test/', transform=transform)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def filter_dataset(dataset, selected_classes):\n","    # Filter the dataset samples based on the selected class IDs\n","    filtered_samples = [(img, label) for img, label in dataset.samples if label in selected_classes]\n","    # Update the dataset's samples and targets\n","    dataset.samples = filtered_samples\n","    dataset.targets = [label for _, label in filtered_samples]\n","    # Update the class_to_idx attribute based on the filtered classes\n","    dataset.class_to_idx = {class_name: idx for class_name, idx in dataset.class_to_idx.items() if idx in selected_classes}\n","    return dataset\n","\n","selected_classes = [0, 2, 4, 7, 9]\n","\n","train_ds = filter_dataset(train_ds, selected_classes)\n","valid_ds = filter_dataset(valid_ds, selected_classes)\n","test_ds = filter_dataset(test_ds, selected_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Number of train samples: \", len(train_ds))\n","print(\"Number of test samples: \", len(test_ds))\n","print(\"Detected Classes are: \", train_ds.class_to_idx)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import ViTModel\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Define the model\n","class ViTForImageClassification(nn.Module):\n","    def __init__(self, num_labels, other_argument=None):\n","        super(ViTForImageClassification, self).__init__()\n","        # Your model initialization code here\n","        # Example:\n","        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n","        self.dropout = nn.Dropout(0.1)\n","        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n","        self.num_labels = num_labels\n","\n","    def forward(self, pixel_values, labels=None):\n","        # Your forward pass code here\n","        # Example:\n","        outputs = self.vit(pixel_values=pixel_values)\n","        output = self.dropout(outputs.last_hidden_state[:, 0])\n","        logits = self.classifier(output)\n","\n","        if labels is not None:\n","            # Calculate loss\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            return loss\n","        else:\n","            return logits\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["EPOCHS = 12\n","BATCH_SIZE = 64\n","LEARNING_RATE = 2e-5"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import ViTFeatureExtractor\n","import torch.nn as nn\n","import torch\n","# Define Model\n","model = ViTForImageClassification(len(train_ds.classes))    \n","# Feature Extractor\n","feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n","# Adam Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","# Cross Entropy Loss\n","loss_func = nn.CrossEntropyLoss()\n","# Use GPU if available  \n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n","if torch.cuda.is_available():\n","    model.cuda() "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch.utils.data as data\n","from torch.autograd import Variable\n","import numpy as np\n","from tqdm import tqdm\n","from torchvision.transforms.functional import to_pil_image\n","\n","\n","train_loader = data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n","test_loader = data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n","\n","# Training loop\n","for epoch in range(EPOCHS):\n","    # Use tqdm to track progress\n","    with tqdm(train_loader, unit=\"batch\") as t:\n","        for i, (images, labels) in enumerate(t):\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = loss_func(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            if (i+1) % 100 == 0:\n","                t.set_description(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f}')\n","    \n","    # Evaluate on test set\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for x, y in test_loader:\n","            x, y = x.to(device), y.to(device)\n","            output = model(x)\n","            preds = output.argmax(dim=1)\n","            correct += (preds == y).sum().item()\n","            total += y.size(0)\n","    accuracy = correct / total\n","    print(f'Epoch: {epoch+1} | Test Accuracy: {accuracy:.4f}')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.save(model, './models/model_ViT.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","def evaluate(model, test_dataloader, class_names):\n","    y_true = []\n","    y_pred = []\n","    with torch.no_grad():\n","        for data in test_dataloader:\n","            images, labels = data\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            c = (predicted == labels).squeeze()\n","            for i in range(len(labels)):   \n","                label = labels[i]\n","                # For confusion matrix\n","                y_true.append(label.to('cpu'))\n","                y_pred.append(predicted[i].to('cpu'))\n","\n","    # Classification report\n","    print(classification_report(y_true, y_pred, target_names=class_names))\n","\n","    # Confusion matrix\n","    matrix = confusion_matrix(y_true, y_pred)\n","    disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=[\"BCM\", \"CR\", \"KTE\", \"OO\", \"RP\"])\n","    disp.plot()\n","    plt.title(\"Confusion Matrix\")\n","    plt.show()\n","\n","# Usage\n","evaluate(model, test_loader, [\"Bak Chor Mee\", \"Chicken Rice\", \"Kaya Toast and Egg\", \"Oyster Omelette\", \"Roti Prata\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import random\n","\n","def display_random_images(model, test_loader, class_names):\n","    model.eval()\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            for i in range(len(labels)):\n","                if random.random() < 5 / len(test_loader.dataset):\n","                    label = labels[i].item()\n","                    plt.imshow(images[i].permute(1, 2, 0).cpu().numpy())\n","                    plt.title(f\"True: {class_names[label]}, Predicted: {class_names[predicted[i].item()]}\")\n","                    plt.axis('off')\n","                    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["display_random_images(model, test_loader, [\"Bak Chor Mee\", \"Chicken Rice\", \"Kaya Toast and Egg\", \"Oyster Omelette\", \"Roti Prata\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","EVAL_BATCH = 1\n","eval_loader  = data.DataLoader(valid_ds, batch_size=EVAL_BATCH, shuffle=True, num_workers=4) \n","# Disable grad\n","with torch.no_grad():\n","    \n","  inputs, target = next(iter(eval_loader))\n","  # Reshape and get feature matrices as needed\n","  print(inputs.shape)\n","  inputs = inputs[0].permute(1, 2, 0)\n","  # Save original Input\n","  originalInput = inputs\n","  for index, array in enumerate(inputs):\n","    inputs[index] = np.squeeze(array)\n","  inputs = torch.tensor(np.stack(feature_extractor(inputs)['pixel_values'], axis=0))\n","\n","  # Send to appropriate computing device\n","  inputs = inputs.to(device)\n","  target = target.to(device)\n"," \n","  # Generate prediction\n","  prediction, loss = model(inputs, target)\n","    \n","  # Predicted class value using argmax\n","  predicted_class = np.argmax(prediction.cpu())\n","  value_predicted = list(valid_ds.class_to_idx.keys())[list(valid_ds.class_to_idx.values()).index(predicted_class)]\n","  value_target = list(valid_ds.class_to_idx.keys())[list(valid_ds.class_to_idx.values()).index(target)]\n","        \n","  # Show result\n","  plt.imshow(originalInput)\n","  plt.xlim(224,0)\n","  plt.ylim(224,0)\n","  plt.title(f'Prediction: {value_predicted} - Actual target: {value_target}')\n","  plt.show()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4707001,"sourceId":7994861,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelInstanceId":18704,"sourceId":22567,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":18711,"sourceId":22577,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":22827,"sourceId":27091,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":4}
