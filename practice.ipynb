{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["from __future__ import print_function, division\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","\n","import torchvision.transforms.functional as F\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, Subset\n","import numpy as np\n","import torchvision\n","from torchvision import models, transforms\n","from torchvision.datasets.folder import make_dataset\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","from sklearn.model_selection import GridSearchCV\n","from __future__ import print_function, division\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","\n","import torchvision.transforms.functional as F\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, Subset\n","import numpy as np\n","import torchvision\n","from torchvision import models, transforms\n","from torchvision.datasets.folder import make_dataset\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report\n","from torchsummary import summary\n","import timm\n","from timm.loss import LabelSmoothingCrossEntropy # This is better than normal nn.CrossEntropyLoss\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","%matplotlib inline\n","plt.ion()   # interactive mode"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["seed = 42\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Loading data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define the dataset class\n","class sg_food_dataset(torch.utils.data.dataset.Dataset):\n","    def __init__(self, root, class_id, transform=None):\n","        self.class_id = class_id\n","        self.root = root\n","        all_classes = sorted(entry.name for entry in os.scandir(root) if entry.is_dir())\n","        if not all_classes:\n","            raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n","        self.classes = [all_classes[x] for x in class_id]\n","        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n","\n","        self.samples = make_dataset(self.root, self.class_to_idx, extensions=('jpg'))\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        path, target = self.samples[idx]\n","        with open(path, \"rb\") as f:\n","            sample = Image.open(f).convert('RGB')\n","        if self.transform is not None:\n","            sample = self.transform(sample)\n","        return sample, target\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Data augmentation and normalization for training\n","data_transforms = {\n","    'train': transforms.Compose([\n","        # Define data preparation operations for training set here.\n","        # Tips: Use torchvision.transforms\n","        #       https://pytorch.org/vision/stable/transforms.html\n","        #       Normally this should at least contain resizing (Resize) and data format converting (ToTensor).\n","        transforms.RandomResizedCrop(224),\n","        transforms.ColorJitter(brightness=0.1, contrast=0.1 , saturation = 0.1), #random brightness, contrast etc\n","        transforms.GaussianBlur(kernel_size=(15, 15)),\n","        transforms.ColorJitter(brightness=0.2, contrast=0.3 , saturation = 0.2, hue=0.3), #random brightness, contrast etc\n","        transforms.RandomVerticalFlip(),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet prior\n","    ]),\n","    'val': transforms.Compose([\n","        # Define data preparation operations for testing/validation set here.\n","        transforms.Resize(256, antialias=True),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet prior\n","    ]),\n","    'test': transforms.Compose([\n","        # Define data preparation operations for testing/validation set here.\n","        transforms.Resize((256,256), antialias=True),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet prior\n","    ]),\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","data_dir = './sg_food' \n","subfolder = {'train': 'train', 'val': 'val'}\n","\n","# Define the dataset\n","selected_classes = [0, 2, 4, 7, 9]\n","n_classes = len(selected_classes)\n","image_datasets = {x: sg_food_dataset(root=os.path.join(data_dir, subfolder[x]),\n","                                     class_id=selected_classes,\n","                                     transform=data_transforms[x]) \n","                  for x in ['train', 'val', 'test']}\n","class_names = image_datasets['train'].classes\n","print('selected classes:\\n    id: {}\\n    name: {}'.format(selected_classes, class_names))\n","\n","# Define the dataloader\n","batch_size = 64\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n","                                             shuffle=True, num_workers=0)\n","              for x in ['train', 'val', 'test']}\n","\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Visualizing the dataset\n","Fetch a batch of training data from the dataset and visualize them. \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def imshow(inp, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    inp = std * inp + mean\n","    inp = np.clip(inp, 0, 1)\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","\n","\n","# Get a batch of training data\n","inputs, classes = next(iter(dataloaders['train']))\n","\n","# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs[:4])\n","\n","imshow(out, title=[class_names[x] for x in classes[:4]])"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Model Initailization"]},{"cell_type":"markdown","metadata":{},"source":["### 3.1 Define Models"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load the pre-trained VGG16 model\n","vgg16 = models.vgg16(pretrained=True)\n","\n","# Load the pre-trained VGG19 model\n","vgg19 = models.vgg19(pretrained=True)\n","\n","# Load the pre-trained ResNet-18 model\n","resnet18 = models.resnet18(pretrained=True)\n","\n","# Load the pre-trained ResNet-50 model\n","resnet50 = models.resnet50(pretrained=True)\n","\n","# Load the pre-trained Inception v3 model\n","inception_v3 = models.inception_v3(pretrained=True)\n"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2 Modify Models"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Set num of classes\n","num_classes = 5\n","\n","# Modify VGG16\n","vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, num_classes)\n","\n","# Modify VGG19\n","vgg19.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, num_classes)\n","\n","# Modify ResNet18\n","resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n","\n","# Modify Resnet50\n","resnet50.fc = nn.Linear(resnet50.fc.in_features, num_classes)\n","\n","# Modify Inception_v3\n","inception_v3.fc = nn.Linear(inception_v3.fc.in_features, num_classes)\n","inception_v3.AuxLogits.fc = nn.Linear(inception_v3.AuxLogits.fc.in_features, num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Summary\n","\n","# VGG16\n","summary(vgg16.to(device), input_size=(3, 224, 224))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# VGG19\n","summary(vgg19.to(device), input_size=(3, 224, 224))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# ResNet18\n","summary(resnet18.to(device), input_size=(3, 224, 224))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Resnet 50\n","summary(resnet50.to(device), input_size=(3, 224, 224))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Inception_v3\n","summary(inception_v3.to(device), input_size=(3, 299, 299))"]},{"cell_type":"markdown","metadata":{},"source":["### 3.3 Dict for Models"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["models_dict = {\n","    \"VGG16\": vgg16,\n","    \"VGG19\": vgg19,\n","    \"ResNet18\": resnet18,\n","    \"ResNet50\":resnet50,\n","    \"Inception_v3\": inception_v3\n","}"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train_model(model, dataloaders, criterion, optimizer, num_epochs, is_inception=False):\n","    since = time.time()\n","\n","    val_acc_history = []\n","    \n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0                \n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                \n","                if is_inception and phase == 'train':\n","                    inputs = torch.stack([F.resize(input, size=(299, 299)) for input in inputs])\n","\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    # Special case for inception because in training it has an auxiliary output. In train\n","                    #   mode we calculate the loss by summing the final output and the auxiliary output\n","                    #   but in testing we only consider the final output.\n","                    if is_inception and phase == 'train':\n","                        outputs, aux_outputs = model(inputs)\n","                        loss1 = criterion(outputs, labels)\n","                        loss2 = criterion(aux_outputs, labels)\n","                        loss = loss1 + 0.4*loss2\n","                    else:\n","                        outputs = model(inputs)\n","                        loss = criterion(outputs, labels)\n","\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.detch().item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","            if phase == 'val':\n","                val_acc_history.append(epoch_acc)\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, val_acc_history\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def visualize_training(val_acc_history):\n","    plt.figure()\n","    # 确保将 tensor 转移到 CPU\n","    val_acc_history_cpu = [h.cpu() for h in val_acc_history]\n","    plt.plot(val_acc_history_cpu)\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Validation Accuracy\")\n","    plt.title(\"Training History\")\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Loop through each model for training\n","for model_name, model in models_dict.items():\n","    print(f\"Training {model_name}...\")\n","    \n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    model = model.to(device)\n","    \n","    # Define Criterion\n","    criterion = nn.CrossEntropyLoss()\n","    \n","    # Define Optimizer\n","    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","    \n","    # Inception_v3\n","    is_inception = (model_name == \"Inception_v3\")\n","    \n","    trained_model, val_acc_history = train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=is_inception)\n","    \n","    # Visualize Training Result\n","    visualize_training(val_acc_history)\n","    \n","    # Save Checkpoints\n","    torch.save(trained_model.state_dict(), f\"./model/{model_name}_model.pth\")"]},{"cell_type":"markdown","metadata":{},"source":["##############################\n","这里是evaluate5个模型的部分\n","##############################"]},{"cell_type":"markdown","metadata":{},"source":["advance：yaoxuan"]},{"cell_type":"markdown","metadata":{},"source":["**Train Swin Transformer (tiny) with lr scheduler**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["HUB_URL = \"SharanSMenon/swin-transformer-hub:main\"\n","MODEL_NAME = \"swin_tiny_patch4_window7_224\"\n","# check hubconf for more models.\n","model = torch.hub.load(HUB_URL, MODEL_NAME, pretrained=True) # load from torch hub"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for param in model.parameters(): #freeze model\n","    param.requires_grad = False\n","print(model.head)\n","n_inputs = model.head.in_features\n","model.head = nn.Sequential(\n","    nn.Linear(n_inputs, 512),\n","    nn.ReLU(),\n","    nn.Dropout(0.3),\n","    nn.Linear(512, 5)\n",")\n","model = model.to(device)\n","print(model.head)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["criterion = LabelSmoothingCrossEntropy()\n","criterion = criterion.to(device)\n","optimizer = optim.AdamW(model.head.parameters(), lr=0.001)\n","# lr scheduler\n","exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.97)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs):\n","    since = time.time()\n","\n","    val_acc_history = []\n","    \n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0                \n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    loss = criterion(outputs, labels)\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","            \n","            if phase == 'train':\n","                scheduler.step()\n","                \n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","            if phase == 'val':\n","                val_acc_history.append(epoch_acc)\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, val_acc_history\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trained_model, val_acc_history = train_model(model, dataloaders, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n","    \n","# Visualize Training Result\n","visualize_training(val_acc_history)\n","    \n","# Save Checkpoints\n","torch.save(trained_model.state_dict(), f\"/kaggle/working/swin_tiny.pth\")"]},{"cell_type":"markdown","metadata":{},"source":["# 5. Prediction and Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load the pre-trained Swin_tiny model\n","HUB_URL = \"SharanSMenon/swin-transformer-hub:main\"\n","MODEL_NAME = \"swin_tiny_patch4_window7_224\"\n","swin_tiny = torch.hub.load(HUB_URL, MODEL_NAME, pretrained=True)\n","\n","# Load the pre-trained VGG16 model\n","vgg16 = models.vgg16(pretrained=True)\n","\n","# Load the pre-trained ResNet-18 model\n","resnet18 = models.resnet18(pretrained=True)\n","\n","# Load the pre-trained Inception v3 model\n","inception_v3 = models.inception_v3(pretrained=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Set num of classes\n","num_classes = 5\n","\n","# Modify VGG16\n","vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, num_classes)\n","\n","# Modify ResNet18\n","resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n","\n","# Modify Inception_v3\n","inception_v3.fc = nn.Linear(inception_v3.fc.in_features, num_classes)\n","inception_v3.AuxLogits.fc = nn.Linear(inception_v3.AuxLogits.fc.in_features, num_classes)\n","\n","# Modify Swin_tiny\n","swin_tiny.head = nn.Sequential(\n","    nn.Linear(768, 512),\n","    nn.ReLU(),\n","    nn.Dropout(0.3),\n","    nn.Linear(512, num_classes)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["models_dict = {\n","    \"VGG16\": vgg16,\n","    \"ResNet18\": resnet18,\n","    \"Inception_v3\": inception_v3,\n","    \"SwinTransformer_tiny\": swin_tiny\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Data normalization for testing\n","test_data_transform = transforms.Compose([\n","        # Define data preparation operations for testing/validation set here.\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet prior\n","    ])\n","\n","data_dir = '/kaggle/input/sg-food/sg_food' \n","\n","# Define the dataset\n","selected_classes = [0, 2, 4, 7, 9]\n","n_classes = len(selected_classes)\n","test_image_dataset = sg_food_dataset(root=os.path.join(data_dir, 'test'),\n","                                     class_id=selected_classes,\n","                                     transform=test_data_transform) \n","class_names = test_image_dataset.classes\n","print('selected classes:\\n    id: {}\\n    name: {}'.format(selected_classes, class_names))\n","\n","# Define the dataloader\n","batch_size = 64\n","test_dataloader = torch.utils.data.DataLoader(test_image_dataset, batch_size=batch_size,\n","                                             shuffle=True, num_workers=0)\n","\n","test_dataset_size = len(test_image_dataset)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def imshow(inp, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    inp = std * inp + mean\n","    inp = np.clip(inp, 0, 1)\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","\n","\n","# Get a batch of training data\n","inputs, classes = next(iter(test_dataloader))\n","\n","# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs[:4])\n","\n","imshow(out, title=[class_names[x] for x in classes[:4]])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["best_model_paths = {\n","    \"VGG16\": \"/kaggle/input/VGG16/pytorch/v1/1/VGG16_model.pth\",\n","    \"ResNet18\": \"/kaggle/input/resnet18-v1/pytorch/v1/1/ResNet18_model.pth\",\n","    \"Inception_v3\": \"/kaggle/input/inception_v3/pytorch/v1/1/Inception_v3_model.pth\",\n","    \"SwinTransformer_tiny\": \"/kaggle/input/swin_tiny/pytorch/v1/1/swin_tiny.pth\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","def evaluate(model,test_dataloader,class_names):\n","    y_true = []\n","    y_pred = []\n","    y_false_pred = []\n","    y_false_img = []\n","    with torch.no_grad():\n","        for data in test_dataloader:\n","            images, labels = data\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            for i in range(len(labels)):   \n","                label = labels[i]\n","                #for confusion matrix\n","                y_true.append(label.to('cpu'))\n","                y_pred.append(predicted[i].to('cpu'))\n","                #record false predictions\n","                if label != predicted[i]:\n","                    y_false_pred.append(f\"Ground Truth:{class_names[label.to('cpu')]}; Predicted: {class_names[predicted[i].to('cpu')]}\")\n","                    y_false_img.append(images[i].to('cpu'))\n","                \n","        \n","    print(classification_report(y_true, y_pred, target_names=class_names))\n","    \n","    matrix = confusion_matrix(y_true, y_pred)\n","    disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=[\"BCM\",\"CR\",\"KTE\",\"OO\",\"RP\"])\n","    disp.plot()\n","    plt.title(\"confusion matrix\")\n","    plt.show()\n","    \n","    \n","    imgs = torchvision.utils.make_grid(y_false_img[:4])\n","    imshow(imgs, title=y_false_pred[:4])\n","    \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for model_name, model in models_dict.items():\n","    if model_name == \"VGG16\":\n","        continue\n","    print(f\"Testing {model_name}...\")\n","    \n","    model.to(device)\n","    model.load_state_dict(torch.load(best_model_paths[model_name]))\n","    model.eval()\n","    evaluate(model,test_dataloader,class_names)\n","    "]},{"cell_type":"markdown","metadata":{},"source":["### advanced 1"]},{"cell_type":"markdown","metadata":{},"source":["### Threshold"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load ResNet18\n","model = models.resnet18(pretrained=False)\n","\n","# Modify Model, Remove FC Layer\n","num_ftrs = model.fc.in_features\n","model.fc = torch.nn.Linear(num_ftrs, 5)\n","\n","# Load pretrained weight\n","model.load_state_dict(torch.load(\"./model/ResNet18_model.pth\", map_location=torch.device('cpu')),strict = False)\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def predict_with_confidence(model, image_tensor, threshold=0.5):\n","    with torch.no_grad():  # 不计算梯度\n","        outputs = model(image_tensor)\n","        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n","        max_prob, preds = torch.max(probabilities, 1)\n","        if max_prob.item() < threshold:\n","            return 'Unknown'\n","        else:\n","            return f'Class {preds.item()} with confidence {max_prob.item()}'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_transforms = transforms.Compose([\n","    transforms.Resize(256, antialias=True),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","def preprocess_image(image_path):\n","    image = Image.open(image_path).convert('RGB')  # 确保转换为RGB\n","    return val_transforms(image).unsqueeze(0)  # 添加批次维度，因为模型期待批次作为第一维度\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 假设你已经有了一个图像路径\n","image_path = '/Users/pineapple/Desktop/4414/IE4414_F34_Grp7/sg_food/train/Hokkien Prawn Mee/Hokkien Prawn Mee(57).jpg'\n","image_tensor = preprocess_image(image_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","prediction = predict_with_confidence(model, image_tensor, threshold=0.7)\n","print(prediction)"]},{"cell_type":"markdown","metadata":{},"source":["### SVM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class sg_food_dataset(torch.utils.data.Dataset):\n","    def __init__(self, root, class_id, transform=None, include_others=False):\n","        self.class_id = class_id\n","        self.root = root\n","        all_classes = sorted(entry.name for entry in os.scandir(root) if entry.is_dir())\n","        if not all_classes:\n","            raise FileNotFoundError(f\"Couldn't find any class folder in {root}.\")\n","\n","        self.classes = [all_classes[x] for x in class_id] + (['others'] if include_others else [])\n","        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n","        others_index = self.class_to_idx.get('others', len(self.classes))  # 默认为最后一个索引\n","        \n","        self.samples = []\n","        for entry in os.scandir(root):\n","            if entry.is_dir():\n","                dir_index = self.class_to_idx.get(entry.name, others_index)\n","                for img_entry in os.scandir(entry.path):\n","                    if img_entry.name.endswith(('jpg', 'jpeg', 'png')) and (dir_index != others_index or include_others):\n","                        self.samples.append((img_entry.path, dir_index))\n","\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        path, target = self.samples[idx]\n","        with open(path, \"rb\") as f:\n","            sample = Image.open(f).convert('RGB')\n","        if self.transform is not None:\n","            sample = self.transform(sample)\n","        return sample, target\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_dir = './sg_food' \n","subfolder = {'train': 'train', 'val': 'val'}\n","selected_classes = [0, 2, 4, 7, 9]  # 选定的5个类别\n","\n","# Define the dataset with the others category included\n","image_datasets = {\n","    x: sg_food_dataset(\n","        root=os.path.join(data_dir, subfolder[x]),\n","        class_id=selected_classes,\n","        transform=data_transforms[x],\n","        include_others=True if x == 'train' else False  # 只在训练集中包括 'others' 类别\n","    ) for x in ['train', 'val']\n","}\n","\n","# 数据加载器\n","dataloaders = {\n","    x: torch.utils.data.DataLoader(image_datasets[x], batch_size=64, shuffle=True, num_workers=0)\n","    for x in ['train', 'val']\n","}\n","\n","class_names = image_datasets['train'].classes\n","print('Selected classes with \"others\":', class_names)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 加载预训练的ResNet18模型\n","model = models.resnet18(pretrained=False)\n","num_ftrs = model.fc.in_features\n","model.fc = torch.nn.Identity()  # 将全连接层替换为Identity以输出特征\n","\n","# 加载你自己的权重\n","model.load_state_dict(torch.load(\"./model/ResNet18_model.pth\", map_location=torch.device('cpu')), strict=False)\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_features_and_labels(dataloader):\n","    features = []\n","    labels = []\n","    for inputs, label in dataloader:\n","        with torch.no_grad():\n","            outputs = model(inputs)\n","        features.extend(outputs.detach().cpu().numpy())\n","        labels.extend(label.detach().cpu().numpy())\n","    return features, labels"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_features, train_labels = get_features_and_labels(dataloaders['train'])\n","val_features, val_labels = get_features_and_labels(dataloaders['val'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# 定义类权重\n","weights = {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 0.142}  # 假设类5是“others”类\n","\n","# 创建一个管道，包括标准化和带权重的SVC\n","pipeline = make_pipeline(StandardScaler(), SVC(gamma='auto', kernel='rbf', probability=True, class_weight=weights))\n","\n","# 定义参数网格\n","param_grid = {'svc__C': [0.01, 0.1, 1]}\n","\n","# 使用 GridSearchCV 来搜索最佳参数\n","grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n","\n","# 训练模型\n","grid_search.fit(train_features, train_labels)\n","\n","# 打印最佳参数和对应的性能评分\n","print(\"Best parameters:\", grid_search.best_params_)\n","print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import classification_report, accuracy_score\n","import seaborn as sb\n","\n","\n","# 使用最佳模型对验证集进行预测\n","val_predictions = grid_search.best_estimator_.predict(val_features)\n","\n","# 计算并打印准确率\n","val_accuracy = accuracy_score(val_labels, val_predictions)\n","print(\"Validation Accuracy: {:.2f}\".format(val_accuracy))\n","\n","# 打印详细的分类报告\n","print(classification_report(val_labels, val_predictions))\n","\n","cm = confusion_matrix(val_labels, val_predictions)\n","plt.figure(figsize=(10,7))\n","sb.heatmap(cm, annot=True, fmt='d')\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4707001,"sourceId":7994861,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelInstanceId":18704,"sourceId":22567,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":18711,"sourceId":22577,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":22827,"sourceId":27091,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":4}
