{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7994861,"sourceType":"datasetVersion","datasetId":4707001},{"sourceId":22567,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":18704},{"sourceId":22577,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":18711},{"sourceId":27091,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":22827}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n\nimport torchvision.transforms.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Subset\nimport numpy as np\nimport torchvision\nfrom torchvision import models, transforms\nfrom torchvision.datasets.folder import make_dataset\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\n\n%matplotlib inline\nplt.ion()   # interactive mode","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install torchsummary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Loading data\n","metadata":{}},{"cell_type":"code","source":"# Define the dataset class\nclass sg_food_dataset(torch.utils.data.dataset.Dataset):\n    def __init__(self, root, class_id, transform=None):\n        self.class_id = class_id\n        self.root = root\n        all_classes = sorted(entry.name for entry in os.scandir(root) if entry.is_dir())\n        if not all_classes:\n            raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n        self.classes = [all_classes[x] for x in class_id]\n        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n\n        self.samples = make_dataset(self.root, self.class_to_idx, extensions=('jpg'))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        path, target = self.samples[idx]\n        with open(path, \"rb\") as f:\n            sample = Image.open(f).convert('RGB')\n        if self.transform is not None:\n            sample = self.transform(sample)\n        return sample, target\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data augmentation and normalization for training\ndata_transforms = {\n    'train': transforms.Compose([\n        # Define data preparation operations for training set here.\n        # Tips: Use torchvision.transforms\n        #       https://pytorch.org/vision/stable/transforms.html\n        #       Normally this should at least contain resizing (Resize) and data format converting (ToTensor).\n        transforms.RandomResizedCrop(224),\n        transforms.ColorJitter(brightness=0.1, contrast=0.1 , saturation = 0.1), #random brightness, contrast etc\n        transforms.GaussianBlur(kernel_size=(15, 15)),\n        transforms.ColorJitter(brightness=0.2, contrast=0.3 , saturation = 0.2, hue=0.3), #random brightness, contrast etc\n        transforms.RandomVerticalFlip(),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet prior\n    ]),\n    'val': transforms.Compose([\n        # Define data preparation operations for testing/validation set here.\n        transforms.Resize(256, antialias=True),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet prior\n    ]),\n}\n\ndata_dir = '/kaggle/input/sg-food/sg_food' \nsubfolder = {'train': 'train', 'val': 'val'}\n\n# Define the dataset\nselected_classes = [0, 2, 4, 7, 9]\nn_classes = len(selected_classes)\nimage_datasets = {x: sg_food_dataset(root=os.path.join(data_dir, subfolder[x]),\n                                     class_id=selected_classes,\n                                     transform=data_transforms[x]) \n                  for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\nprint('selected classes:\\n    id: {}\\n    name: {}'.format(selected_classes, class_names))\n\n# Define the dataloader\nbatch_size = 64\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n                                             shuffle=True, num_workers=0)\n              for x in ['train', 'val']}\n\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Visualizing the dataset\nFetch a batch of training data from the dataset and visualize them. \n\n","metadata":{}},{"cell_type":"code","source":"def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs[:4])\n\nimshow(out, title=[class_names[x] for x in classes[:4]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Model Initailization","metadata":{}},{"cell_type":"markdown","source":"### 3.1 Define Models","metadata":{}},{"cell_type":"code","source":"# Load the pre-trained VGG16 model\nvgg16 = models.vgg16(pretrained=True)\n\n# Load the pre-trained ResNet-18 model\nresnet18 = models.resnet18(pretrained=True)\n\n# Load the pre-trained Inception v3 model\ninception_v3 = models.inception_v3(pretrained=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Modify Models","metadata":{}},{"cell_type":"code","source":"# Set num of classes\nnum_classes = 5\n\n# Modify VGG16\nvgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, num_classes)\n\n# Modify ResNet18\nresnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n\n# Modify Inception_v3\ninception_v3.fc = nn.Linear(inception_v3.fc.in_features, num_classes)\ninception_v3.AuxLogits.fc = nn.Linear(inception_v3.AuxLogits.fc.in_features, num_classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summary\n\n# VGG16\nsummary(vgg16.to(device), input_size=(3, 224, 224))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ResNet18\nsummary(resnet18.to(device), input_size=(3, 224, 224))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inception_v3\nsummary(inception_v3.to(device), input_size=(3, 299, 299))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3 Dict for Models","metadata":{}},{"cell_type":"code","source":"models_dict = {\n    \"VGG16\": vgg16,\n    \"ResNet18\": resnet18,\n    \"Inception_v3\": inception_v3\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Training","metadata":{}},{"cell_type":"code","source":"def train_model(model, dataloaders, criterion, optimizer, num_epochs, is_inception=False):\n    since = time.time()\n\n    val_acc_history = []\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0                \n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                \n                if is_inception and phase == 'train':\n                    inputs = torch.stack([F.resize(input, size=(299, 299)) for input in inputs])\n\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    # Special case for inception because in training it has an auxiliary output. In train\n                    #   mode we calculate the loss by summing the final output and the auxiliary output\n                    #   but in testing we only consider the final output.\n                    if is_inception and phase == 'train':\n                        outputs, aux_outputs = model(inputs)\n                        loss1 = criterion(outputs, labels)\n                        loss2 = criterion(aux_outputs, labels)\n                        loss = loss1 + 0.4*loss2\n                    else:\n                        outputs = model(inputs)\n                        loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            if phase == 'val':\n                val_acc_history.append(epoch_acc)\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, val_acc_history\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_training(val_acc_history):\n    plt.figure()\n    # 确保将 tensor 转移到 CPU\n    val_acc_history_cpu = [h.cpu() for h in val_acc_history]\n    plt.plot(val_acc_history_cpu)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Accuracy\")\n    plt.title(\"Training History\")\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loop through each model for training\nfor model_name, model in models_dict.items():\n    print(f\"Training {model_name}...\")\n    \n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    \n    # Define Criterion\n    criterion = nn.CrossEntropyLoss()\n    \n    # Define Optimizer\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n    \n    # Inception_v3\n    is_inception = (model_name == \"Inception_v3\")\n    \n    trained_model, val_acc_history = train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=is_inception)\n    \n    # Visualize Training Result\n    visualize_training(val_acc_history)\n    \n    # Save Checkpoints\n    torch.save(trained_model.state_dict(), f\"/kaggle/working/{model_name}_model.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train Swin Transformer (tiny) with lr scheduler**","metadata":{}},{"cell_type":"code","source":"import timm\nfrom timm.loss import LabelSmoothingCrossEntropy # This is better than normal nn.CrossEntropyLoss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HUB_URL = \"SharanSMenon/swin-transformer-hub:main\"\nMODEL_NAME = \"swin_tiny_patch4_window7_224\"\n# check hubconf for more models.\nmodel = torch.hub.load(HUB_URL, MODEL_NAME, pretrained=True) # load from torch hub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for param in model.parameters(): #freeze model\n    param.requires_grad = False\nprint(model.head)\nn_inputs = model.head.in_features\nmodel.head = nn.Sequential(\n    nn.Linear(n_inputs, 512),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(512, 5)\n)\nmodel = model.to(device)\nprint(model.head)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = LabelSmoothingCrossEntropy()\ncriterion = criterion.to(device)\noptimizer = optim.AdamW(model.head.parameters(), lr=0.001)\n# lr scheduler\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.97)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs):\n    since = time.time()\n\n    val_acc_history = []\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0                \n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n                    _, preds = torch.max(outputs, 1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            \n            if phase == 'train':\n                scheduler.step()\n                \n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            if phase == 'val':\n                val_acc_history.append(epoch_acc)\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, val_acc_history\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model, val_acc_history = train_model(model, dataloaders, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n    \n# Visualize Training Result\nvisualize_training(val_acc_history)\n    \n# Save Checkpoints\ntorch.save(trained_model.state_dict(), f\"/kaggle/working/swin_tiny.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Prediction and Evaluation","metadata":{}},{"cell_type":"code","source":"# Load the pre-trained Swin_tiny model\nHUB_URL = \"SharanSMenon/swin-transformer-hub:main\"\nMODEL_NAME = \"swin_tiny_patch4_window7_224\"\nswin_tiny = torch.hub.load(HUB_URL, MODEL_NAME, pretrained=True)\n\n# Load the pre-trained VGG16 model\nvgg16 = models.vgg16(pretrained=True)\n\n# Load the pre-trained ResNet-18 model\nresnet18 = models.resnet18(pretrained=True)\n\n# Load the pre-trained Inception v3 model\ninception_v3 = models.inception_v3(pretrained=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set num of classes\nnum_classes = 5\n\n# Modify VGG16\nvgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, num_classes)\n\n# Modify ResNet18\nresnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n\n# Modify Inception_v3\ninception_v3.fc = nn.Linear(inception_v3.fc.in_features, num_classes)\ninception_v3.AuxLogits.fc = nn.Linear(inception_v3.AuxLogits.fc.in_features, num_classes)\n\n# Modify Swin_tiny\nswin_tiny.head = nn.Sequential(\n    nn.Linear(768, 512),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(512, num_classes)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_dict = {\n    \"VGG16\": vgg16,\n    \"ResNet18\": resnet18,\n    \"Inception_v3\": inception_v3,\n    \"SwinTransformer_tiny\": swin_tiny\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data normalization for testing\ntest_data_transform = transforms.Compose([\n        # Define data preparation operations for testing/validation set here.\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet prior\n    ])\n\ndata_dir = '/kaggle/input/sg-food/sg_food' \n\n# Define the dataset\nselected_classes = [0, 2, 4, 7, 9]\nn_classes = len(selected_classes)\ntest_image_dataset = sg_food_dataset(root=os.path.join(data_dir, 'test'),\n                                     class_id=selected_classes,\n                                     transform=test_data_transform) \nclass_names = test_image_dataset.classes\nprint('selected classes:\\n    id: {}\\n    name: {}'.format(selected_classes, class_names))\n\n# Define the dataloader\nbatch_size = 64\ntest_dataloader = torch.utils.data.DataLoader(test_image_dataset, batch_size=batch_size,\n                                             shuffle=True, num_workers=0)\n\ntest_dataset_size = len(test_image_dataset)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\ninputs, classes = next(iter(test_dataloader))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs[:4])\n\nimshow(out, title=[class_names[x] for x in classes[:4]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model_paths = {\n    #\"VGG16\": \"/kaggle/input/VGG16/pytorch/v1/1/VGG16_model.pth\",\n    \"ResNet18\": \"/kaggle/input/resnet18-v1/pytorch/v1/1/ResNet18_model.pth\",\n    \"Inception_v3\": \"/kaggle/input/inception_v3/pytorch/v1/1/Inception_v3_model.pth\",\n    \"SwinTransformer_tiny\": \"/kaggle/input/swin_tiny/pytorch/v1/1/swin_tiny.pth\"\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndef evaluate(model,test_dataloader,class_names):\n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for data in test_dataloader:\n            images, labels = data\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            c = (predicted == labels).squeeze()\n            for i in range(len(labels)):   \n                label = labels[i]\n                #for confusion matrix\n                y_true.append(label.to('cpu'))\n                y_pred.append(predicted[i].to('cpu'))\n                \n        \n    print(classification_report(y_true, y_pred, target_names=class_names))\n    \n    matrix = confusion_matrix(y_true, y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=[\"BCM\",\"CR\",\"KTE\",\"OO\",\"RP\"])\n    disp.plot()\n    plt.title(\"confusion matrix\")\n    plt.show()\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model_name, model in models_dict.items():\n    if model_name == \"VGG16\":\n        continue\n    print(f\"Testing {model_name}...\")\n    \n    model.to(device)\n    model.load_state_dict(torch.load(best_model_paths[model_name]))\n    model.eval()\n    evaluate(model,test_dataloader,class_names)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### advanced 1","metadata":{}},{"cell_type":"code","source":"# Load ResNet18\nmodel = models.resnet18(pretrained=False)\n\n# Modify Model, Remove FC Layer\nnum_ftrs = model.fc.in_features\nmodel.fc = torch.nn.Identity()\n\n# Load pretrained weight\nmodel.load_state_dict(torch.load(\"/kaggle/input/resnet18-v1/pytorch/v1/1/ResNet18_model.pth\"),strict = False)\nmodel.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class sg_food_dataset(torch.utils.data.Dataset):\n    def __init__(self, root, class_id, transform=None):\n        self.class_id = class_id\n        self.root = root\n        all_classes = sorted(entry.name for entry in os.scandir(root) if entry.is_dir())\n        if not all_classes:\n            raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n        \n        # 创建新的类别列表，包括指定的类别和一个\"Other\"类别\n        self.classes = [all_classes[x] for x in class_id] + ['Other']\n        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes[:-1])}\n        self.class_to_idx['Other'] = len(self.classes) - 1  # 'Other'类别的索引\n\n        # 更新make_dataset函数调用，以支持\"Other\"类别\n        self.samples = self.make_dataset(self.root, self.class_to_idx, extensions=('jpg'), class_id=class_id)\n        self.transform = transform\n\n    def make_dataset(self, directory, class_to_idx, extensions=None, class_id=None):\n        instances = []\n        directory = os.path.expanduser(directory)\n        both_none = extensions is None\n        if not both_none:\n            def is_valid_file(x):\n                return x.lower().endswith(extensions)\n        for target_class in sorted(class_to_idx.keys()):\n            class_index = class_to_idx[target_class]\n            target_dir = os.path.join(directory, target_class)\n            if not os.path.isdir(target_dir):\n                continue\n            for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n                for fname in sorted(fnames):\n                    path = os.path.join(root, fname)\n                    if both_none or is_valid_file(path):\n                        # 若当前类别不在selected_classes中，将其归为\"Other\"\n                        if class_index not in class_id:\n                            instances.append((path, class_to_idx['Other']))\n                        else:\n                            instances.append((path, class_index))\n        return instances\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# 数据目录和子目录保持不变\ndata_dir = '/kaggle/input/sg-food/sg_food' \nsubfolder = {'train': 'train', 'val': 'val'}\n\n# 选定的类别索引不变，因为sg_food_dataset类内部会处理Other类别\nselected_classes = [0, 2, 4, 7, 9]\n\n# 定义数据集时保持原有逻辑\nimage_datasets = {x: sg_food_dataset(root=os.path.join(data_dir, subfolder[x]),\n                                     class_id=selected_classes,\n                                     transform=data_transforms[x]) \n                  for x in ['train', 'val']}\n\n# 更新类别名称的获取逻辑，现在包括了\"Other\"类别\nclass_names = image_datasets['train'].classes  # 现在这里已经包含了\"Other\"\nprint('Selected classes:\\n    ID: {}\\n    Name: {}'.format(selected_classes + ['Other'], class_names))\n\n# 定义数据加载器和设备选择的逻辑保持不变\nbatch_size = 64\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n                                             shuffle=True, num_workers=0)\n              for x in ['train', 'val']}\n\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}